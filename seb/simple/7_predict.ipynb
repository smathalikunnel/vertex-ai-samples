{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cac3a6-ec1e-4b45-a1b0-45a1862a5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS:vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151191fa-3707-4f77-b9e0-9d48f3cf9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d95574c-e86b-4cf8-9358-da7e14b6b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir \"gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec27892-0ef8-416a-864b-bfd20b283be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir \"gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314c6822-8344-477b-aeb5-a7960cabcaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dropoff_latitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_dropoff_latitude:0\n",
      "  inputs['dropoff_longitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_dropoff_longitude:0\n",
      "  inputs['passenger_count'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_passenger_count:0\n",
      "  inputs['pickup_latitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_pickup_latitude:0\n",
      "  inputs['pickup_longitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_pickup_longitude:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir \"gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4563bf-d4d8-4b95-877a-9facc5daa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58172e1e-bf1f-411b-9c38-28e0b311085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/387138108602/locations/us-central1/models/7304702256153100288/operations/7250018114065137664\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/387138108602/locations/us-central1/models/7304702256153100288\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/387138108602/locations/us-central1/models/7304702256153100288')\n"
     ]
    }
   ],
   "source": [
    "model = aip.Model.upload(\n",
    "    display_name=\"test_keras\",\n",
    "    artifact_uri=\"gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390/model/\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51ea5f1-6d2e-49c3-9a6e-0d6873b3db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5,2016-03-15 07:03:57 UTC,-73.901199340820312,40.85577392578125,-73.904006958007812,40.8521614074707,2,unused\n",
      "2.5,2016-03-24 21:43:21 UTC,-73.990829467773438,40.717990875244141,-73.990562438964844,40.718437194824219,2,unused\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://vertex_e2e_taxi_data/taxifare/data/taxi-valid-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f88df6-bbb5-4d2c-9698-5d0ab6a4f37c",
   "metadata": {},
   "source": [
    "model.batch_predict(\n",
    "    job_display_name=\"test_predict\",\n",
    "    gcs_source=\"gs://vertex_e2e_taxi_data/taxifare/data/taxi-train-000000000000.csv\",\n",
    "    bigquery_destination_prefix=\"vf-grp-commercial-tst-explore.taxifare\",\n",
    "    machine_type=\"n1-standard-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba1456-7d19-4272-8770-8d0b61a0ab81",
   "metadata": {},
   "source": [
    "### X above fails, which is expected (data has inputs plus labels, no headers as reqd by vertex for csv files). But vertex does not show any logs or stacktrace, only 'internal err'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d90ed4-480d-4039-9675-bc3943e739d1",
   "metadata": {},
   "source": [
    "### TODO move below code to common area (or common import) so that same code can be used for training and serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92ba591-9aa6-439a-b211-f049b17081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "CSV_COLUMNS = [\n",
    "    #'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "LABEL_DEFAULT = [0.0]\n",
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "\n",
    "def features_and_labels(row_data, mode='eval'):\n",
    "# The .pop() method will return item and drop from frame. \n",
    "\n",
    "    \n",
    "    features = row_data\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\t\n",
    "    if mode == 'predict':\n",
    "        return features\n",
    "    else:\n",
    "        label = row_data.pop(LABEL_COLUMN)\n",
    "        return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode='eval'):\n",
    "# The tf.data.experimental.make_csv_dataset() method reads CSV files into a dataset\n",
    "    if mode == 'predict':\n",
    "        dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    \n",
    "    else: \n",
    "    \tdataset = tf.data.experimental.make_csv_dataset(pattern, batch_size, [LABEL_COLUMN]+CSV_COLUMNS, LABEL_DEFAULT+DEFAULTS)\n",
    "\n",
    "# The map() function executes a specified function for each item in an iterable.\n",
    "# The item is sent to the function as a parameter.\n",
    "    dataset = dataset.map(lambda x: features_and_labels(x,mode))\n",
    "\n",
    "    if mode == 'train':\n",
    "# The shuffle() method takes a sequence (list, string, or tuple) and reorganize the order of the items.\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dfa2804e-2134-4024-92da-da4c80b169b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_dataset('gs://vertex_e2e_taxi_data/taxifare/data/taxi-train-000000000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0199a1a9-8cb0-4479-ace3-a506dae7b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ds = create_dataset('gs://vertex_e2e_taxi_data/taxifare/data/taxi-pred-000000000000.csv', mode='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebbc513-8dac-4bd8-99ce-1f43d8c09f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 11:17:40.811329: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.756546], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.98264], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.75752], dtype=float32),\n",
      " 'pickup_longitude': array([-73.97189], dtype=float32)}\n",
      "\n",
      "\n",
      "{'dropoff_latitude': array([40.78193], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.95166], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.740658], dtype=float32),\n",
      " 'pickup_longitude': array([-73.98184], dtype=float32)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pprint\n",
    "for data in pred_ds.take(2):\n",
    "\tpprint.pprint({k: v.numpy() for k,v in data.items()})\n",
    "\t\n",
    "        #pprint({k:v.numpy() for k,v in data})\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3aea918e-2d09-487b-9fb3-7a928c838f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.82963], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.94097], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.82564], dtype=float32),\n",
      " 'pickup_longitude': array([-73.94363], dtype=float32)}\n",
      "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>\n",
      "\n",
      "\n",
      "{'dropoff_latitude': array([40.75132], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.99037], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.7575], dtype=float32),\n",
      " 'pickup_longitude': array([-73.99053], dtype=float32)}\n",
      "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for data in test_ds.take(2):\n",
    "\tpprint.pprint({k: v.numpy() for k,v in data[0].items()})\n",
    "\tpprint.pprint(data[1])\n",
    "        #pprint({k:v.numpy() for k,v in data})\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2520772-b07a-4e72-aebe-bf1015dcbc27",
   "metadata": {},
   "source": [
    "### why 2? expected behavior was to return size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c84e5d3c-b3b7-4a1c-9514-bc4a2402abdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e484d3-dbcc-4f15-844a-ae1911bec657",
   "metadata": {},
   "source": [
    "data for prediction -\n",
    "1. will not have labels. can the training dataset handle that?\n",
    "2. vertex model object can load csv with headers and create request. but data processing needs to happen on that data (since processing code is not part of model like in keras).\n",
    "3. hence, prediction flow seems to be to xform data , save it in a BQ/GCS bucket. vertex model prediction will pick it up from there\n",
    "\n",
    "\n",
    "|                |             |                        |\n",
    "|   pred_raw     |             |     pred_processed     |\n",
    "|________________|---xform---->|________________________|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc51ab-d221-4cb6-a65e-2fdb69aad4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(pred_ds, 'gs://vertex_e2e_taxi_data/taxifare/data/pred_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51b3ea1-abf2-4031-b2db-ea0a1e994743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: OrderedDict([(pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (passenger_count, (1,))]), types: OrderedDict([(pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (passenger_count, tf.float32)])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70473207-fbde-4e64-af0f-d44fcc8bd238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: OrderedDict([(pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (passenger_count, (1,))]), types: OrderedDict([(pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (passenger_count, tf.float32)])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset#snapshot\n",
    "pred_ds.snapshot(path='gs://vertex_e2e_taxi_data/taxifare/data/pred_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3026006-339e-4222-ad67-b5b6ca323620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: OrderedDict([(pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (passenger_count, (1,))]), types: OrderedDict([(pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (passenger_count, tf.float32)])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ds.snapshot(path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5d849f-628a-4944-9bcd-ba04a129b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_keras_sequential_api.ipynb\t    6_replace_with_gcs_data.ipynb  taxi_trained\n",
      "4_run_in_vertex.ipynb\t\t    7_predict.ipynb\t\t   train.py\n",
      "5_export_data_from_bq_to_gcs.ipynb  data\t\t\t   train_gcs.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65664a11-1c2f-432b-b569-1d442b6ce143",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch header.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d06d60-f84e-4b4f-8740-e447990c299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "tfio.experimental.serialization.save_dataset(\n",
    "    pred_ds,'gs://vertex_e2e_taxi_data/taxifare/data/pred_processed.csv', './header.yamlb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fed9fc8-256c-4c56-a4f7-098636c906ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pickup_longitude': {'shape': [1], 'dtype': tf.float32}, 'pickup_latitude': {'shape': [1], 'dtype': tf.float32}, 'dropoff_longitude': {'shape': [1], 'dtype': tf.float32}, 'dropoff_latitude': {'shape': [1], 'dtype': tf.float32}, 'passenger_count': {'shape': [1], 'dtype': tf.float32}}\n"
     ]
    }
   ],
   "source": [
    "header = {}\n",
    "for key in pred_ds.element_spec.keys():\n",
    "    header[key] = {\n",
    "        \"shape\": list(pred_ds.element_spec[key].shape),\n",
    "        \"dtype\": pred_ds.element_spec[key].dtype,\n",
    "    }\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c01a22f0-136d-47e3-b17f-1388183d7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.779636], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.957726], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.756348], dtype=float32),\n",
      " 'pickup_longitude': array([-73.97449], dtype=float32)}\n",
      "{'dropoff_latitude': array([40.7513], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.97813], dtype=float32),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_latitude': array([40.74467], dtype=float32),\n",
      " 'pickup_longitude': array([-73.981], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "for x in pred_ds.take(2):\n",
    "    # Each individual tensor is converted to a known serializable type.\n",
    "    features = {key: value.numpy() for key, value in x.items()}\n",
    "    pprint.pprint(features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45852447-ba05-49d4-94e6-de4dd34116dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-74.00917], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.7108], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98931], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.75371], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>)])\n"
     ]
    }
   ],
   "source": [
    "for item in pred_ds.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44708975-0e39-4c52-9aff-2c55aa2fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(row_dict):\n",
    "  \"\"\"\n",
    "  Creates a tf.train.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type.\n",
    "  #feature = {\n",
    "  #    'feature0': _int64_feature(feature0),\n",
    "  #    'feature1': _int64_feature(feature1),\n",
    "  #    'feature2': _bytes_feature(feature2),\n",
    "  #    'feature3': _float_feature(feature3),\n",
    "  # }\n",
    "  feature = {key: _float_feature(value.numpy()) for key, value in row_dict.items()}\n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57bdb6e2-1c1e-43ff-a9ce-49143cdc834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\x95\\x01\\n\\x1b\\n\\x0fpassenger_count\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00@\\n\\x1d\\n\\x11dropoff_longitude\\x12\\x08\\x12\\x06\\n\\x04\\xbe\\xeb\\x93\\xc2\\n\\x1c\\n\\x10dropoff_latitude\\x12\\x08\\x12\\x06\\n\\x04\\xfc\\x1a#B\\n\\x1c\\n\\x10pickup_longitude\\x12\\x08\\x12\\x06\\n\\x04\\xc2\\xf4\\x93\\xc2\\n\\x1b\\n\\x0fpickup_latitude\\x12\\x08\\x12\\x06\\n\\x04\\xfe\\x1b#B'\n",
      "features {\n",
      "  feature {\n",
      "    key: \"dropoff_latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.77635192871094\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"dropoff_longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -73.96043395996094\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"passenger_count\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 2.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup_latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.77733612060547\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup_longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -73.97804260253906\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in pred_ds.take(1):\n",
    "    ser_string = serialize_example(item)\n",
    "    print(ser_string)\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(ser_string)\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fbbd62-f31a-4aa6-ade6-edf5c7431e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "  for features in pred_ds:\n",
    "    yield serialize_example(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a892c24-0a0c-4c89-bf59-143efdfb7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_features_dataset = tf.data.Dataset.from_generator(\n",
    "    generator, output_types=tf.string, output_shapes=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a30c1e-d21a-4008-98cf-e6f65d488043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce670fe-025a-4a2e-b557-fe0ac41f32cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\x95\\x01\\n\\x1c\\n\\x10pickup_longitude\\x12\\x08\\x12\\x06\\n\\x04~\\xdd\\x93\\xc2\\n\\x1d\\n\\x11dropoff_longitude\\x12\\x08\\x12\\x06\\n\\x04\\xb9\\xd1\\x93\\xc2\\n\\x1c\\n\\x10dropoff_latitude\\x12\\x08\\x12\\x06\\n\\x04\\x95\\xb9\"B\\n\\x1b\\n\\x0fpickup_latitude\\x12\\x08\\x12\\x06\\n\\x04\\xfc\\xc9\"B\\n\\x1b\\n\\x0fpassenger_count\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00@', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in serialized_features_dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7f4dc-29a5-4d4c-ac97-eccf159bb981",
   "metadata": {},
   "source": [
    "## below took few hours to execute?\n",
    "how can we make this faster, and run on vertex instead of nb cpu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d541b0-a69d-4f72-b019-d02c2d56e084",
   "metadata": {},
   "source": [
    "filename = 'pred.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e064272c-7d04-479c-969a-c7313d4bf356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"dropoff_latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.78719711303711\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"dropoff_longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -73.9517822265625\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"passenger_count\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 2.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup_latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.72761535644531\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup_longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -73.98524475097656\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "{'pickup_longitude': array([-73.98524475]), 'passenger_count': array([2.]), 'dropoff_longitude': array([-73.95178223]), 'dropoff_latitude': array([40.78719711]), 'pickup_latitude': array([40.72761536])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 22:35:42.276910: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "filename = 'pred.tfrecord'\n",
    "#read\n",
    "raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "raw_dataset\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  print(example)\n",
    "\n",
    "  result = {}\n",
    "  # example.features.feature is the dictionary\n",
    "  for key, feature in example.features.feature.items():\n",
    "    # The values are the Feature objects which contain a `kind` which contains:\n",
    "    # one of three fields: bytes_list, float_list, int64_list\n",
    "\n",
    "    kind = feature.WhichOneof('kind')\n",
    "    result[key] = np.array(getattr(feature, kind).value)\n",
    "\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732b0fb-1e15-47f1-b59a-6c65a917bbad",
   "metadata": {},
   "source": [
    "### try to make batch prediction using 1 example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0cb86b3-a59c-4882-b156-3f8fb75c72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1/3054110103.py:2: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    }
   ],
   "source": [
    "filename = 'pred_1.tfrecord' \n",
    "writer = tf.data.experimental.TFRecordWriter(filename) \n",
    "writer.write(serialized_features_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8987404-ffb5-4d50-aeea-9106314bcc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://pred_1.tfrecord [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  168.0 B/  168.0 B]                                                \n",
      "Operation completed over 1 objects/168.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp pred_1.tfrecord gs://vertex_e2e_taxi_data/taxifare/data/pred_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93fe6635-e63e-4232-8d04-1c4c8b076afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/6585016920192319488?project=387138108602\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/387138108602/locations/us-central1/batchPredictionJobs/6585016920192319488 current state:\n",
      "JobState.JOB_STATE_FAILED\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 13\nmessage: \"INTERNAL\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/545616442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minstances_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf-record\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbigquery_destination_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vf-grp-commercial-tst-explore.taxifare\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n1-standard-2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mbatch_predict\u001b[0;34m(self, job_display_name, gcs_source, bigquery_source, instances_format, gcs_destination_prefix, bigquery_destination_prefix, predictions_format, model_parameters, machine_type, accelerator_type, accelerator_count, starting_replica_count, max_replica_count, generate_explanation, explanation_metadata, explanation_parameters, labels, credentials, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   2202\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m             \u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m         )\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, job_display_name, model_name, instances_format, predictions_format, gcs_source, bigquery_source, gcs_destination_prefix, bigquery_destination_prefix, model_parameters, machine_type, accelerator_type, accelerator_count, starting_replica_count, max_replica_count, generate_explanation, explanation_metadata, explanation_parameters, labels, project, location, credentials, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mgca_batch_prediction_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgapic_batch_prediction_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mgenerate_explanation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_explanation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, empty_batch_prediction_job, model_or_model_name, gca_batch_prediction_job, generate_explanation, sync)\u001b[0m\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mbatch_prediction_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_prediction_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_JOB_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 13\nmessage: \"INTERNAL\"\n"
     ]
    }
   ],
   "source": [
    "model.batch_predict(\n",
    "    job_display_name=\"batch-predict-1-instance\",\n",
    "    gcs_source=\"gs://vertex_e2e_taxi_data/taxifare/data/pred_results/pred_1.tfrecord\",\n",
    "    instances_format=\"tf-record\",\n",
    "    bigquery_destination_prefix=\"vf-grp-commercial-tst-explore.taxifare\",\n",
    "    machine_type=\"n1-standard-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4a023-9553-43e8-bd08-de6275d0d3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
