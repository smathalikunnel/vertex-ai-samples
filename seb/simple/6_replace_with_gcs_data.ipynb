{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75509958-b9ca-4223-ae55-fcecb62748de",
   "metadata": {},
   "source": [
    "### replace local data path to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc08ee3-ffe8-4958-ac20-abbce5deab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_gcs.py\n",
    "\n",
    "\n",
    "# The datetime module used to work with dates as date objects.\n",
    "import datetime\n",
    "# The OS module in python provides functions for interacting with the operating system.\n",
    "import os\n",
    "# The shutil module in Python provides many functions of high-level operations on files and collections of files.\n",
    "# This module helps in automating process of copying and removal of files and directories.\n",
    "import shutil\n",
    "\n",
    "# Here we'll import data processing libraries like Numpy, Pandas and Tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import pyplot package from matplotlib library\n",
    "from matplotlib import pyplot as plt\n",
    "# Import keras package from tensorflow library\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import Sequential function from tensorflow.keras.models\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Import Dense, DenseFeatures function from tensorflow.keras.layers\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures\n",
    "# Import TensorBoard function from tensorflow.keras.callbacks\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "# The .pop() method will return item and drop from frame. \n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    # feat engg\n",
    "    #row based and full-pass\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode='eval'):\n",
    "# The tf.data.experimental.make_csv_dataset() method reads CSV files into a dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "# The map() function executes a specified function for each item in an iterable.\n",
    "# The item is sent to the function as a parameter.\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "\n",
    "    if mode == 'train':\n",
    "# The shuffle() method takes a sequence (list, string, or tuple) and reorganize the order of the items.\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "# Defining the feature names into a list `INPUT_COLS`\n",
    "INPUT_COLS = [\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "]\n",
    "\n",
    "# Create input layer of feature columns\n",
    "# TODO 1\n",
    "feature_columns = {\n",
    "    colname: tf.feature_column.numeric_column(colname)\n",
    "    for colname in INPUT_COLS\n",
    "    }\n",
    "# Build a keras DNN model using Sequential API\n",
    "# TODO 2a\n",
    "model = Sequential([\n",
    "    DenseFeatures(feature_columns=feature_columns.values()),\n",
    "    Dense(units=32, activation=\"relu\", name=\"h1\"),\n",
    "    Dense(units=8, activation=\"relu\", name=\"h2\"),\n",
    "    Dense(units=1, activation=\"linear\", name=\"output\")\n",
    "    ])\n",
    "# TODO 2b\n",
    "# Create a custom evalution metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "TRAIN_BATCH_SIZE = 1000\n",
    "#this is equal to no. of training data (10000) * epochs (5)\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  # training dataset will repeat, wrap around\n",
    "#evaluation and checkpointing happens at the end of each epoch, we want 50 evaluations intead of original 5\n",
    "#hence we need to make epocs =50\n",
    "#but we dont want the data to repeat 50 times. hence we need to decrease the no. of steps_per_epoch aaccordingly\n",
    "NUM_EVALS = 50  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern='gs://vertex_e2e_taxi_data/taxifare/data/taxi-train-000000000000.csv',\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    mode='train')\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern='gs://vertex_e2e_taxi_data/taxifare/data/taxi-valid-000000000000.csv',\n",
    "    batch_size=1000,\n",
    "    mode='eval').take(NUM_EVAL_EXAMPLES//1000)\n",
    "\n",
    "\n",
    "# TODO 3\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "LOGDIR = \"./taxi_trained\"\n",
    "# Train the sequential model\n",
    "history = model.fit(x=trainds,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=NUM_EVALS,\n",
    "                    validation_data=evalds,\n",
    "                    callbacks=[TensorBoard(LOGDIR)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0757d3-691d-4f75-9610-79176174a40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae445bb6-2e10-45f3-899c-2546b029d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477b8399-4240-456e-b7e8-f5d028927ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gs://vertex_e2e_example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44748465-ffcf-47da-a2bf-4a0030076f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    #display_name=JOB_NAME,\n",
    "    display_name=\"model on vertex - gcs data\",\n",
    "    script_path=\"train_gcs.py\",\n",
    "    #container_uri=TRAIN_IMAGE,\n",
    "    container_uri=\"europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\",\n",
    "    #requirements=[\"google-cloud-bigquery>=2.20.0\"],\n",
    "    #model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    staging_bucket=bucket,\n",
    "    project='vf-grp-commercial-tst-explore',\n",
    "    location='europe-west1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec3dda1-f754-411d-a64d-87a5b6026b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://vertex_e2e_example/aiplatform-2021-11-01-17:50:23.036-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-17:50:23.284 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/4039816826681556992?project=387138108602\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/8281996569432031232?project=387138108602\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/387138108602/locations/europe-west1/trainingPipelines/4039816826681556992 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "       # dataset=dataset,\n",
    "       # model_display_name=MODEL_DISPLAY_NAME,\n",
    "       # bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "       # args=CMDARGS,\n",
    "        replica_count=1,\n",
    "       # machine_type=TRAIN_COMPUTE,\n",
    "    machine_type='n1-standard-4',\n",
    "        accelerator_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaca6871-3b29-4d65-a7a0-1e925744da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train_gcs.py\n",
    "\n",
    "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2463ae-e887-4e20-a0ed-bf147b0f1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    #display_name=JOB_NAME,\n",
    "    display_name=\"model on vertex - gcs data\",\n",
    "    script_path=\"train_gcs.py\",\n",
    "    #container_uri=TRAIN_IMAGE,\n",
    "    container_uri=\"europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\",\n",
    "    #requirements=[\"google-cloud-bigquery>=2.20.0\"],\n",
    "    #model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    staging_bucket=bucket,\n",
    "    project='vf-grp-commercial-tst-explore',\n",
    "    location='europe-west1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e27ce2-c425-45f8-8967-965b3aeb4780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://vertex_e2e_example/aiplatform-2021-11-01-18:04:55.134-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertex_e2e_example/aiplatform-custom-training-2021-11-01-18:04:55.390 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/5323131614249615360?project=387138108602\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/1621172720551067648?project=387138108602\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/387138108602/locations/europe-west1/trainingPipelines/5323131614249615360 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "       # dataset=dataset,\n",
    "       # model_display_name=MODEL_DISPLAY_NAME,\n",
    "       # bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "       # args=CMDARGS,\n",
    "        replica_count=1,\n",
    "       # machine_type=TRAIN_COMPUTE,\n",
    "    machine_type='n1-standard-4',\n",
    "        accelerator_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bcf49-cd71-492d-a206-67a053076101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
